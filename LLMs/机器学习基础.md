# 1.机器学习中常用的ensemble方法有哪些？
- voting：多个模型采取投票的形式，投票数最多者确定为最终的分类。（分类任务）
- averaging：对于一些回归任务，将多个模型的结果进行加权平均，具体的权重可以根据模型效果的排名进行确定。效果越好的模型，权重设置也越大，反之越小。
- bagging（减小方差）：每次从原始训练集中抽取n个样本，共抽取k次，得到k个训练集（有放回的抽取）。用这些训练集去训练k个模型，对于分类任务，采取投票的方式，对于回归任务，取k个模型的均值作为结果。
- boosting（减小偏差）：每一轮的训练集不变，首先训练一个base模型，然后根据这个模型对训练样本的预测正确率，进行权重的重新分配，就比如分类错误的那些样本用更多的权重。重复这个训练过程n次，不断强化模型对错误样本的预测能力。（如果放到分类任务中，可以多次采样那些预测错误的数据，进行模型的训练）（模型的训练整体来说是串行的）
- stacking：首先将数据平均分成多分，训练多个不同类型的基模型，然后用这些模型去预测剩余的数据，将这些模型的结果作为输入特征，重新训练一个型的模型。
也就是说，在实际过程中，我们预测一个样本，首先需要几个基学习器进行预测，得到几个结果，然后将这个结果作为输入，让我们训练的高级模型去进行预测得到最终的结果。
优点：训练出的高级模型，能够利用不同基模型的优点，
缺点：计算的复杂度过高。
# 2.l1正则化、l2正则化相关内容。
- 解释l1和l2的时候，涉及到一个等值线的内容，损失函数的等值线就是具有同样一个损失值的参数组合。
- 那么从图像的方面解释就是l1会使权重中有很多0，l2的话就是会使权重比较小。二者都起到了正则化的作用。