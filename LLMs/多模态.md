# 1.ALBEF详解
![Alt](assert/albef.png#pic_center)
# 2.BLIP详解
![Alt](assert/blip.png#pic_center)
- 其实一共就有两个模型，一个是vit作为img encoder，另外一个是BERT以及BERT的变体，只不过是最后输出的head不同。
- 这三个损失是同时计算的，也就是img通过vit进行特征提取，然后text分别经过三次bert(实际上两次就可以了)之后计算三个损失函数。
- 也维护一个基模型，每次的参数是新模型的参数和原始模型滑动更新的结果。
### 1.损失
- ITC和CLIP一样，就是对齐文本和图像信息，拉进二者之间的距离。
- ITM其实是一个二分类问题，判断text和img是否匹配，这里利用了hard negatives，就是维护一个队列，然后每次从这里面挑选出一些相似度高的作为负样本。
- LM就是用一个用人工标注的数据去训练，网络针对一张图片生成对应的文本输出。
### 2.数据方面
首先用大量的人工标注数据，去训练模型，得到一个基础模型。
然后利用互联网上爬取的数据，首先通过ITM判断图文是否匹配，若不匹配，则让模型生成一个新的描述。通过这样操作去构建一个新的大型数据集。用这个数据集去重新训练一个模型。
# 3.BLIP2
![Alt](assert/blip2.png#pic_center)
- 设置32维的可学习query，和实际的text文本拼接在一起，然后不同部分采用不同的mask进行后续损失的计算。
- image提取到的特征做cross attention，和learn query计算注意力。
- 这里感觉是将图像信息压缩到了32维的query中。
### 1.预训练query
- ==ITM==：判断text和img的匹配程度，计算这个的时候，文本和query可以互相看到。(应该是一个二分类)
- ==ITC==：文本和query互不可见。
- ==IGT==：query完全可见，但是text是每次只能看到他后面的词。
### 2.query联合LLM
![Alt](assert/blip2-1.png#pic_center)
- Decoder-only：Q-Former使用image feature作cross attention，输入设置为learned query，并且通过一个全连接层将维度转为LLM的输入维度。
- Encoder-Decoder：learned query和一部分text作为query和image feature做cross attention，
### 3.VQA-fintune
![Alt](assert/blip2-2.png#pic_center)
query + question一起作为q，和image feature 做cross attention，之后query在和question一起作为LLM的输入，从而得到最后的结果。