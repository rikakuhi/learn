# 1.masked attention的实现
- 网络输出的digit，用-100替换掉原本的数，然后进行softmax计算。因为-100作为指数，计算出来的数值会非常小，从而相当于变为0了。
# 2.BERT
- 具体架构：12、24层transformer-encoder
- 预训练：
    - 在输入的时候，第一个token是CLS，两个句子之间是SEP用于将两个句子隔开
    - 输出的时候第一个token是NSP(用于判断两个句子是否是前后句的关系) 
    - 第一个任务MLM：将输入中的15%的token替换为MASK，然后再输出的地方预测原本的词，从而做交叉熵损失。80%替换为MASK，10%替换为其他的toekn(随机)，10%的概率保持不变。
    - 第二个任务：判断两个句子是否存在前后文关系，在训练的时候A句子和B句子有50%的概率存在前后文关系。
    - Embedding相关：Position Embedding(learnable position embedding) + Token Embedding + Segment Embedding(句子A和句子B不同，用于区分两个句子：一般初始化为0/1)均是可学习的embedding
- finetune：
    - 对于分类任务，只需要将text输入，然后提取CLS部分对应的输出，然后通过一个全连接层进行分类。
    - 对于多分类问题，则连接多个全连接层然后进行分类，将所有的Loss加在一起，进行反向传播。

# 3.clip text encoder的具体架构（CLIP的具体架构）
- Image Encoder：分别用了ResNet50和ViT，提取得到图片的向量表示。
- Text Encoder: 用的是BERT，同样得到的是文字的向量表示。
- 针对二者维度不同的情况，会在得到图片或者文字的向量之后，在后面加上一全连接层，目的是将其转换到相同的维度，并且分别进行l2归一化 。
- 损失函数：将归一化之后的两组特征，计算相似度(然后这里训练了一个可学习的参数对相似度进行缩放，将logits进行放大，这样区分度可能会好一些), 之后设置一个对角线的label，因为对角线的图片和文字是互相对应的，然后分别和图像、文字计算交叉熵损失，两个损失相加/2得到最后结果。
- 下游任务：
    - 图片分类：输入一张图片和多个类别text(实验验证加上 a photo of class效果更好)，之后计算这张图片和得到的text向量相似度最大的那个。
    - 图像检索：输入text，然后一直输入image，然后找到和text的相似度超过阈值的。
# 4.交叉熵的优缺点、L1损失和L2损失的优缺点
- 交叉熵：
    - 只专注学习正确类的标签，因为其他的概率都是0，计算出来全部都是0。
    - 当模型效果好的时候，学习的慢，反之学习的快。
- L1 Loss
    - 导数是个常量，当损失值很小的时候，梯度仍然比较大，会有很大的震荡。
- L2 Loss
    - 有多个离群点的时候，造成的损失很大，会使网络向着这个方向学习。
# 5.